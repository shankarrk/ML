{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pycm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_uniqueCounts(df):\n",
    "    for name in df.columns:\n",
    "        print(f'{name} : {df[name].nunique()}')\n",
    "\n",
    "def to_categorical(df, col_name):\n",
    "    df[col_name] = df[col_name].astype('category')\n",
    "    \n",
    "def to_str(df, col_name):\n",
    "    df[col_name] = df[col_name].astype('str')\n",
    "    \n",
    "def fprint(comment, val):\n",
    "    print(f'{comment}:{val}')\n",
    "    \n",
    "def transform_data(tfidf, df, col):\n",
    "    features = tfidf.transform(df[col])\n",
    "    return pd.DataFrame(features.todense(), columns = tfidf.get_feature_names())\n",
    "\n",
    "def featurize_str(df:pd.DataFrame, col:str, inplace:bool=False):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_col = f'name_{col}'\n",
    "    df[stemmed_col] = df[col].map(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "    tvec = TfidfVectorizer(min_df=.0025, max_df=.1, stop_words='english', ngram_range=(1,2))\n",
    "    tvec.fit(df[stemmed_col].dropna())\n",
    "    features = transform_data(tvec, df, stemmed_col)\n",
    "    df.drop(columns=stemmed_col, inplace=True)\n",
    "    if inplace:\n",
    "        index_name = df.index.name\n",
    "        features[index_name] = df.index.values\n",
    "        features.set_index(index_name, inplace=True)\n",
    "        df.drop(columns=col, inplace=True)\n",
    "        features = pd.merge(df, features, left_index=True, right_index=True)\n",
    "    return features\n",
    "\n",
    "def num_encode(df):\n",
    "    cat_cols = df.select_dtypes(['category']).columns\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: x.cat.codes)\n",
    "    \n",
    "def to_numeric(df):\n",
    "    cat_cols = df.select_dtypes(['int8', 'int16', 'int64']).columns\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: x.astype('float64'))\n",
    "    \n",
    "Categories = ['Unknown', 'Free', 'Cheap', 'Average', 'Expensive', 'Luxury']\n",
    "\n",
    "def price_category(price):\n",
    "    cat = 0\n",
    "    if price == 0.0:\n",
    "        cat = 1\n",
    "    elif price > 0.0 and price < 69.000000:\n",
    "        cat = 2\n",
    "    elif price > 69.000000 and price < 106.000000:\n",
    "        cat = 3\n",
    "    elif price > 106.000000 and price < 175.000000:\n",
    "        cat = 4\n",
    "    elif price > 175.000000:\n",
    "        cat = 5\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv('./data/AB_NYC_2019.csv')\n",
    "df.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data prep\n",
    "df['last_review'] = pd.to_datetime(df['last_review'], format='%Y-%m-%d')\n",
    "df['last_review'].fillna(df['last_review'].mode()[0].date())\n",
    "df['last_review_year'] = df['last_review'].dt.year\n",
    "df['last_review_month'] = df['last_review'].dt.month\n",
    "df['last_review_day'] = df['last_review'].dt.day\n",
    "\n",
    "to_str(df, 'name')\n",
    "to_str(df, 'host_name')\n",
    "\n",
    "#Set label\n",
    "df['price_category'] = df['price'].apply(lambda x: price_category(x))\n",
    "\n",
    "to_categorical(df, 'neighbourhood_group')\n",
    "to_categorical(df, 'minimum_nights')\n",
    "to_categorical(df, 'room_type')\n",
    "to_categorical(df, 'neighbourhood')\n",
    "to_categorical(df, 'calculated_host_listings_count')\n",
    "to_categorical(df, 'last_review_year')\n",
    "to_categorical(df, 'last_review_month')\n",
    "to_categorical(df, 'last_review_day')\n",
    "\n",
    "#Fill-up null values\n",
    "df['reviews_per_month'].fillna(df['reviews_per_month'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unneeded columns\n",
    "df.drop(columns=['last_review', 'host_id', 'price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical encoding of categorical columns\n",
    "num_encode(df)\n",
    "to_numeric(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Feature engineering\n",
    "df = featurize_str(df, 'name', inplace=True)\n",
    "df = featurize_str(df, 'host_name', inplace=True)\n",
    "#df.drop(columns=['name', 'host_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train / test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df.pop('price_category')\n",
    "X = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resampling\n",
    "smote = SMOTE('minority')\n",
    "X_sm, y_sm = smote.fit_sample(X_train, y_train)\n",
    "X_train, y_train = X_sm, y_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.62234\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[2]\tvalid_0's multi_logloss: 1.56665\n",
      "[3]\tvalid_0's multi_logloss: 1.51795\n",
      "[4]\tvalid_0's multi_logloss: 1.48125\n",
      "[5]\tvalid_0's multi_logloss: 1.44766\n",
      "[6]\tvalid_0's multi_logloss: 1.41483\n",
      "[7]\tvalid_0's multi_logloss: 1.3822\n",
      "[8]\tvalid_0's multi_logloss: 1.35445\n",
      "[9]\tvalid_0's multi_logloss: 1.32887\n",
      "[10]\tvalid_0's multi_logloss: 1.30775\n",
      "[11]\tvalid_0's multi_logloss: 1.28692\n",
      "[12]\tvalid_0's multi_logloss: 1.26624\n",
      "[13]\tvalid_0's multi_logloss: 1.24713\n",
      "[14]\tvalid_0's multi_logloss: 1.22935\n",
      "[15]\tvalid_0's multi_logloss: 1.21459\n",
      "[16]\tvalid_0's multi_logloss: 1.19994\n",
      "[17]\tvalid_0's multi_logloss: 1.18604\n",
      "[18]\tvalid_0's multi_logloss: 1.17289\n",
      "[19]\tvalid_0's multi_logloss: 1.16248\n",
      "[20]\tvalid_0's multi_logloss: 1.15229\n",
      "[21]\tvalid_0's multi_logloss: 1.14117\n",
      "[22]\tvalid_0's multi_logloss: 1.13141\n",
      "[23]\tvalid_0's multi_logloss: 1.12163\n",
      "[24]\tvalid_0's multi_logloss: 1.11296\n",
      "[25]\tvalid_0's multi_logloss: 1.10476\n",
      "[26]\tvalid_0's multi_logloss: 1.09712\n",
      "[27]\tvalid_0's multi_logloss: 1.08981\n",
      "[28]\tvalid_0's multi_logloss: 1.08333\n",
      "[29]\tvalid_0's multi_logloss: 1.07716\n",
      "[30]\tvalid_0's multi_logloss: 1.07132\n",
      "[31]\tvalid_0's multi_logloss: 1.06602\n",
      "[32]\tvalid_0's multi_logloss: 1.06081\n",
      "[33]\tvalid_0's multi_logloss: 1.05578\n",
      "[34]\tvalid_0's multi_logloss: 1.05201\n",
      "[35]\tvalid_0's multi_logloss: 1.04756\n",
      "[36]\tvalid_0's multi_logloss: 1.04357\n",
      "[37]\tvalid_0's multi_logloss: 1.04011\n",
      "[38]\tvalid_0's multi_logloss: 1.03644\n",
      "[39]\tvalid_0's multi_logloss: 1.03347\n",
      "[40]\tvalid_0's multi_logloss: 1.03005\n",
      "[41]\tvalid_0's multi_logloss: 1.02651\n",
      "[42]\tvalid_0's multi_logloss: 1.02354\n",
      "[43]\tvalid_0's multi_logloss: 1.02067\n",
      "[44]\tvalid_0's multi_logloss: 1.0181\n",
      "[45]\tvalid_0's multi_logloss: 1.01589\n",
      "[46]\tvalid_0's multi_logloss: 1.0136\n",
      "[47]\tvalid_0's multi_logloss: 1.01143\n",
      "[48]\tvalid_0's multi_logloss: 1.00934\n",
      "[49]\tvalid_0's multi_logloss: 1.00723\n",
      "[50]\tvalid_0's multi_logloss: 1.00536\n",
      "[51]\tvalid_0's multi_logloss: 1.00355\n",
      "[52]\tvalid_0's multi_logloss: 1.00208\n",
      "[53]\tvalid_0's multi_logloss: 1.00082\n",
      "[54]\tvalid_0's multi_logloss: 0.999435\n",
      "[55]\tvalid_0's multi_logloss: 0.998002\n",
      "[56]\tvalid_0's multi_logloss: 0.99667\n",
      "[57]\tvalid_0's multi_logloss: 0.995485\n",
      "[58]\tvalid_0's multi_logloss: 0.994195\n",
      "[59]\tvalid_0's multi_logloss: 0.993278\n",
      "[60]\tvalid_0's multi_logloss: 0.992066\n",
      "[61]\tvalid_0's multi_logloss: 0.991059\n",
      "[62]\tvalid_0's multi_logloss: 0.99027\n",
      "[63]\tvalid_0's multi_logloss: 0.989566\n",
      "[64]\tvalid_0's multi_logloss: 0.988884\n",
      "[65]\tvalid_0's multi_logloss: 0.988356\n",
      "[66]\tvalid_0's multi_logloss: 0.987728\n",
      "[67]\tvalid_0's multi_logloss: 0.987259\n",
      "[68]\tvalid_0's multi_logloss: 0.986487\n",
      "[69]\tvalid_0's multi_logloss: 0.986105\n",
      "[70]\tvalid_0's multi_logloss: 0.985512\n",
      "[71]\tvalid_0's multi_logloss: 0.9851\n",
      "[72]\tvalid_0's multi_logloss: 0.984558\n",
      "[73]\tvalid_0's multi_logloss: 0.984029\n",
      "[74]\tvalid_0's multi_logloss: 0.98381\n",
      "[75]\tvalid_0's multi_logloss: 0.983583\n",
      "[76]\tvalid_0's multi_logloss: 0.983217\n",
      "[77]\tvalid_0's multi_logloss: 0.982911\n",
      "[78]\tvalid_0's multi_logloss: 0.982693\n",
      "[79]\tvalid_0's multi_logloss: 0.982505\n",
      "[80]\tvalid_0's multi_logloss: 0.982346\n",
      "[81]\tvalid_0's multi_logloss: 0.982306\n",
      "[82]\tvalid_0's multi_logloss: 0.982151\n",
      "[83]\tvalid_0's multi_logloss: 0.982092\n",
      "[84]\tvalid_0's multi_logloss: 0.982146\n",
      "[85]\tvalid_0's multi_logloss: 0.982053\n",
      "[86]\tvalid_0's multi_logloss: 0.981677\n",
      "[87]\tvalid_0's multi_logloss: 0.981744\n",
      "[88]\tvalid_0's multi_logloss: 0.981624\n",
      "[89]\tvalid_0's multi_logloss: 0.981535\n",
      "[90]\tvalid_0's multi_logloss: 0.981437\n",
      "[91]\tvalid_0's multi_logloss: 0.981463\n",
      "[92]\tvalid_0's multi_logloss: 0.981445\n",
      "[93]\tvalid_0's multi_logloss: 0.981448\n",
      "[94]\tvalid_0's multi_logloss: 0.981315\n",
      "[95]\tvalid_0's multi_logloss: 0.981124\n",
      "[96]\tvalid_0's multi_logloss: 0.981204\n",
      "[97]\tvalid_0's multi_logloss: 0.981286\n",
      "[98]\tvalid_0's multi_logloss: 0.981318\n",
      "[99]\tvalid_0's multi_logloss: 0.981427\n",
      "[100]\tvalid_0's multi_logloss: 0.981574\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's multi_logloss: 0.981124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1ce610a80b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclassova',\n",
    "    'num_leaves': 100,\n",
    "    'num_class': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       0.80      0.01      0.02       351\n",
      "        Free       0.00      0.00      0.00         3\n",
      "       Cheap       0.69      0.82      0.75      3597\n",
      "     Average       0.51      0.47      0.49      3620\n",
      "   Expensive       0.51      0.46      0.48      3445\n",
      "      Luxury       0.68      0.73      0.71      3653\n",
      "\n",
      "    accuracy                           0.61     14669\n",
      "   macro avg       0.53      0.42      0.41     14669\n",
      "weighted avg       0.60      0.61      0.59     14669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "y_pred = []\n",
    "\n",
    "for x in pred:\n",
    "    y_pred.append(np.argmax(x))\n",
    "    \n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=Categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
